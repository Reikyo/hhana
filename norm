#!/usr/bin/env python

from mva.cmd import get_parser

args = get_parser(actions=False).parse_args()

from mva.analysis import get_analysis

analysis = get_analysis(args)

from rootpy.io import root_open
from rootpy.plotting import Hist
from rootpy.stats import histfactory
from mva.plotting import draw_channel
from mva.categories import Category_Rest, Category_Preselection
from mva.lumi import get_lumi_uncert
from mva.norm import cache
from mva.variables import VARIABLES
from mva import CONST_PARAMS
import numpy as np

lumi_rel_error = get_lumi_uncert(args.year)

# reference category for initial normalization
#category = Category_Rest
category = Category_Preselection

# distribution to fit
field = 'dEta_tau1_tau2'
field_min = 0.
field_max = 2.5
field_bins = 5

# initialize QCD and Ztautau normalizations to 50/50 of data yield
data_yield = analysis.data.events(category, args.target_region)[1].value
ztt_yield = analysis.ztautau.events(category, args.target_region)[1].value
qcd_yield = analysis.qcd.events(category, args.target_region)[1].value

qcd_scale = data_yield / (2 * qcd_yield)
ztt_scale = data_yield / (2 * ztt_yield)

cache.set_scales(
    args.year,
    category.name,
    args.embedding,
    field,
    args.qcd_shape_region,
    qcd_scale=qcd_scale,
    qcd_scale_error=0.,
    qcd_data_scale=1.,
    qcd_z_scale=1.,
    qcd_others_scale=1.,
    z_scale=ztt_scale,
    z_scale_error=0.)

qcd_scale_diff = 100.
ztt_scale_diff = 100.
it = 0
max_iter = 10

while (ztt_scale_diff > 0.01 or qcd_scale_diff > 0.01) and it < max_iter:
    it += 1
    # keep fitting until normalizations converge
    
    analysis.normalize(category)

    hist_template = Hist(field_bins, field_min, field_max, type='D')
    channels = analysis.make_var_channels(
        hist_template, field, [category],
        args.target_region,
        include_signal=False)

    # create a workspace
    measurement = histfactory.make_measurement(
        'normalization_{0}'.format(field), channels,
        POI=None,
        lumi=1.0,
        lumi_rel_error=lumi_rel_error,
        const_params=CONST_PARAMS)
    workspace = histfactory.make_workspace(measurement, silence=True)

    # fit workspace
    minim = workspace.fit(minimizer_type='Minuit', strategy=1)
    fit_result = minim.save()

    # get fitted norms and errors
    qcd = fit_result.floatParsFinal().find('ATLAS_norm_HH_{0:d}_QCD'.format(args.year))
    ztt = fit_result.floatParsFinal().find('ATLAS_norm_HH_{0:d}_Ztt'.format(args.year))
    qcd_scale_new = qcd.getVal()
    qcd_scale_error = qcd.getError()
    ztt_scale_new = ztt.getVal()
    ztt_scale_error = ztt.getError()

    qcd_scale_diff = abs(qcd_scale_new - qcd_scale) / qcd_scale
    ztt_scale_diff = abs(ztt_scale_new - ztt_scale) / ztt_scale
    qcd_scale = qcd_scale_new
    ztt_scale = ztt_scale_new

    # update the cache
    cache.set_scales(
        args.year,
        category.name,
        args.embedding,
        field,
        args.qcd_shape_region,
        qcd_scale=analysis.qcd.scale * qcd_scale,
        qcd_scale_error=analysis.qcd.scale * qcd_scale_error / qcd_scale,
        qcd_data_scale=1.,
        qcd_z_scale=1.,
        qcd_others_scale=1.,
        z_scale=analysis.ztautau.scale * ztt_scale,
        z_scale_error=analysis.ztautau.scale * ztt_scale_error / ztt_scale)

# draw post-fit plots
analysis.normalize(category)

channels = analysis.make_var_channels(
    hist_template, field, [category],
    args.target_region,
    include_signal=False)

draw_channel(channels[0], name=VARIABLES[field]['root'],
    category=category,
    output_name='{0}_postfit{1}'.format(field, analysis.get_suffix()),
    output_dir='plots/normalization')

# determine resonance pt reweighting after normalization
#with root_open('weights.root', 'recreate') as weights_file:
#    resonance_pt_edges = list(np.linspace(0, 250000, 26)) + [1E100]
#    resonance_pt_weight_hist = Hist(resonance_pt_edges, name='resonance_pt_weight')
