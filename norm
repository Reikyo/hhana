#!/usr/bin/env python

from mva.cmd import get_parser

args = get_parser(actions=False).parse_args()

from mva.analysis import get_analysis

analysis = get_analysis(args)

from rootpy.plotting import Hist
from rootpy.stats import histfactory
from mva.plotting import draw_channel
from mva.categories import Category_Rest, Category_Preselection
from mva import norm_cache as cache
from mva.variables import VARIABLES
from mva import CONST_PARAMS

# reference category for initial normalization
#category = Category_Rest
category = Category_Preselection

# distribution to fit
field = 'dEta_tau1_tau2'
field_min = 0.

if args.year == 2011:
    field_max = 2.
    field_bins = 4
else:
    field_max = 2.
    field_bins = 5

# initialize QCD and Ztautau normalizations to 50/50 of data yield
data_yield = analysis.data.events(category, args.target_region)[1].value
ztt_yield = analysis.ztautau.events(category, args.target_region)[1].value
qcd_yield = analysis.qcd.events(category, args.target_region)[1].value

qcd_scale = data_yield / (2 * qcd_yield)
ztt_scale = data_yield / (2 * ztt_yield)

cache.set_scales(
    args.year,
    category.name,
    args.embedding,
    field,
    args.fakes_region,
    qcd_scale=qcd_scale,
    qcd_scale_error=0.,
    qcd_data_scale=1.,
    qcd_z_scale=1.,
    qcd_others_scale=1.,
    z_scale=ztt_scale,
    z_scale_error=0.)

qcd_scale_diff = 100.
ztt_scale_diff = 100.
it = 0
max_iter = 10

while (ztt_scale_diff > 0.01 or qcd_scale_diff > 0.01) and it < max_iter:
    it += 1
    # keep fitting until normalizations converge
    
    analysis.normalize(category)

    hist_template = Hist(field_bins, field_min, field_max, type='D')
    channels = analysis.make_var_channels(
        hist_template, field, [category],
        args.target_region,
        include_signal=False)

    # create a workspace
    measurement = histfactory.make_measurement(
        'normalization_{0}'.format(field), channels,
        POI=None,
        const_params=CONST_PARAMS)
    workspace = histfactory.make_workspace(measurement, silence=True)

    # fit workspace
    minim = workspace.fit()
    fit_result = minim.save()

    # get fitted norms and errors
    qcd = fit_result.floatParsFinal().find('ATLAS_norm_HH_{0:d}_QCD'.format(args.year))
    ztt = fit_result.floatParsFinal().find('ATLAS_norm_HH_{0:d}_Ztt'.format(args.year))
    qcd_scale_new = qcd.getVal()
    qcd_scale_error = qcd.getError()
    ztt_scale_new = ztt.getVal()
    ztt_scale_error = ztt.getError()

    qcd_scale_diff = abs(qcd_scale_new - qcd_scale) / qcd_scale
    ztt_scale_diff = abs(ztt_scale_new - ztt_scale) / ztt_scale
    qcd_scale = qcd_scale_new
    ztt_scale = ztt_scale_new

    # update the cache
    cache.set_scales(
        args.year,
        category.name,
        args.embedding,
        field,
        args.fakes_region,
        qcd_scale=analysis.qcd.scale * qcd_scale,
        qcd_scale_error=analysis.qcd.scale * qcd_scale_error / qcd_scale,
        qcd_data_scale=1.,
        qcd_z_scale=1.,
        qcd_others_scale=1.,
        z_scale=analysis.ztautau.scale * ztt_scale,
        z_scale_error=analysis.ztautau.scale * ztt_scale_error / ztt_scale)

# draw post-fit plots
analysis.normalize(category)

channels = analysis.make_var_channels(
    hist_template, field, [category],
    args.target_region,
    include_signal=False)

draw_channel(channels[0], name=VARIABLES[field]['root'],
    category=category,
    output_name='{0}_postfit{1}'.format(field, analysis.get_suffix()),
    output_dir='plots/normalization',
    show_pvalue=True,
    top_label="Fakes Model: {0}".format(analysis.fakes_region),
    plot_label='After {0} fit'.format(VARIABLES[field]['root']),
    data_info=analysis.data.info,
    output_formats=('eps', 'png'))
