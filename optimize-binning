#!/usr/bin/env python
import os
from math import log
import pickle

from rootpy.plotting import Hist, Canvas
from rootpy.plotting import root2matplotlib as rplt
from rootpy.stats.histfactory import (
    Data, Sample, Channel, make_measurement, make_workspace)

from root_numpy import fill_hist
import numpy as np
import matplotlib.pyplot as plt

from mva import CONST_PARAMS, CACHE_DIR
from mva.categories import Category_VBF, Category_Boosted
from mva.samples import QCD, Ztautau
from mva.analysis import Analysis
from mva.defaults import TARGET_REGION

from statstools.fixups import fix_measurement
from statstools import get_significance_workspace
from statstools.jobs import run_pool

#from joblib import Parallel, delayed
import multiprocessing
from multiprocessing import Process
import time


class SigProcess(Process):
    def __init__(self, *args, **kwargs):
        super(SigProcess, self).__init__()
        self.args = args
        self.kwargs = kwargs
        self.result = multiprocessing.Queue()
    
    @property
    def output(self):
        return self.result.get()

    def run(self):
        self.result.put(get_sig(*self.args, **self.kwargs))


def map_pool(process, args, n_jobs=-1, **kwargs):
    procs = [process(*arg, **kwargs) for arg in args]
    run_pool(procs, n_jobs=n_jobs)
    return [p.output for p in procs]


def get_workspace(scores, binning,
                  mass=125,
                  fill_empties=False,
                  systematics=False):
    hist_template = Hist(binning)
    background = []
    for sample, scores_dict in scores.bkg_scores:
        background.append(sample.get_histfactory_sample(
            hist_template, None, category, TARGET_REGION, scores=scores_dict,
            systematics=systematics))
    signal = []
    for sample, scores_dict in scores.all_sig_scores[mass]:
        signal.append(sample.get_histfactory_sample(
            hist_template, None, category, TARGET_REGION, scores=scores_dict,
            systematics=systematics))
    data_hist = sum([b.hist for b in background])
    data_hist.name = 'Data'
    data = Data('Data', data_hist)
    channel = Channel(category.name, signal + background, data)
    measurement = make_measurement('MVA', channel,
        POI='SigXsecOverSM',
        const_params=CONST_PARAMS)
    fix_measurement(measurement, fill_empties=fill_empties)
    return make_workspace(measurement, silence=False)


def get_sig(scores, binning, edge=None, pos=1,
            mass=125,
            fill_empties=False,
            systematics=False):
    if edge is not None:
        binning = binning[:]
        binning.insert(pos, edge)
    ws = get_workspace(scores, binning,
                       mass=mass,
                       fill_empties=fill_empties,
                       systematics=systematics)
    hist = get_significance_workspace(ws)
    sig = hist[2].value
    # handle nan
    return 0 if sig != sig else sig


def get_best_edge(scores, edges, pos=0,
                  steps=100,
                  min_bkg_unweighted=10,
                  min_bkg_weighted=0,
                  mass=125,
                  systematics=False,
                  n_jobs=-1):
    if pos + 2 == 0:
        left, right = edges[pos:]
    else:
        left, right = edges[pos:pos+2]

    # Only consider edges that give non-negative yields for all backgrounds
    # (negative weights on the background events can be present) and edges that
    # will hold at least min_bkg_unweighted events in the new bin.
    bkg_weighted = []
    bkg_unweighted = []
    sep_sums_weighted = []
    sep_sums_weighted_critical = []
    for bkg, scores_dict in scores.bkg_scores:
        weighted = Hist(steps, left, right)
        unweighted = weighted.Clone()
        s, w = scores_dict['NOMINAL']
        fill_hist(weighted, s, w)
        fill_hist(unweighted, s)
        bkg_weighted.append(weighted)
        bkg_unweighted.append(unweighted)
        # reverse cumsum
        sums = np.cumsum(np.array(list(weighted.y()))[::-1])[::-1]
        sep_sums_weighted.append(sums)
        if isinstance(bkg, (QCD, Ztautau)):
            sep_sums_weighted_critical.append(sums)

    tot_unweighted = sum(bkg_unweighted)
    sum_unweighted = np.cumsum(np.array(list(tot_unweighted.y()))[::-1])[::-1]
    tot_weighted = sum(bkg_weighted)
    sum_weighted = np.cumsum(np.array(list(tot_weighted.y()))[::-1])[::-1]

    # require all backgrounds are nonnegative
    all_nonneg = np.logical_and.reduce([
        b >= 0. for b in sep_sums_weighted])

    # require that critical backgrounds (QCD and Ztt) are positive
    critical_positive = np.logical_and.reduce([
        b > 0. for b in sep_sums_weighted_critical])

    all_positive = np.logical_and(all_nonneg, critical_positive)
    if not np.any(all_positive):
        # not possible to create any new bins
        return None, None, None, None, -1

    sum_unweighted_valid = sum_unweighted >= min_bkg_unweighted
    if not np.any(sum_unweighted_valid):
        # not possible to create any new bins
        return None, None, None, None, -1
    
    sum_weighted_valid = sum_weighted >= min_bkg_weighted
    if not np.any(sum_weighted_valid):
        # not possible to create any new bins
        return None, None, None, None, -1

    # get last bin edge that satisfies background requirements
    last_bin_min_bkg_unweighted = np.where(sum_unweighted_valid)[-1][-1]
    last_bin_min_bkg_weighted = np.where(sum_weighted_valid)[-1][-1]
    last_bin_min_bkg = min(last_bin_min_bkg_unweighted,
                           last_bin_min_bkg_weighted)
    
    last_bin_min_bkg -= all_positive[:last_bin_min_bkg + 1][::-1].argmax()
    last_bin_edge = bkg_weighted[0].xedges(int(last_bin_min_bkg))

    # get significance for each candidate bin edge
    probe_edges = np.linspace(left, last_bin_edge, steps)[1:]
    sigs = map_pool(SigProcess,
                    [(scores, edges, x, pos + 1) for x in probe_edges],
                    mass=mass,
                    systematics=systematics,
                    n_jobs=n_jobs)

    # get best significance and best edge location
    best_sig = np.max(sigs[10:])
    best_edge_idx = np.argmax(sigs[10:]) + 10
    best_edge = probe_edges[best_edge_idx]

    # get unweighted and weighted number of background events in this new bin
    bkg_unweighted = []
    bkg_weighted = []
    for bkg, scores_dict in scores.bkg_scores:
        unweighted = Hist([left, best_edge, right])
        weighted = unweighted.Clone()
        s, w = scores_dict['NOMINAL']
        fill_hist(unweighted, s)
        fill_hist(weighted, s, w)
        bkg_unweighted.append(unweighted)
        bkg_weighted.append(weighted)
    tot_unweighted = sum(bkg_unweighted)
    tot_weighted = sum(bkg_weighted)
    bkg_unweighted_left = tot_unweighted[1].value
    bkg_unweighted_right = tot_unweighted[2].value
    bkg_weighted_left = tot_weighted[1].value
    bkg_weighted_right = tot_weighted[2].value
   
    # Sanity check:
    # the number of background in the higher bin
    # should already never be less than min_bkg due
    # to the numpy magic above
    assert(bkg_unweighted_right >= min_bkg_unweighted)
    assert(bkg_weighted_right >= min_bkg_weighted)

    # stop if lower bin would be too small
    if bkg_unweighted_left < min_bkg_unweighted:
        return None, None, None, None, -1
    if bkg_weighted_left < min_bkg_weighted:
        return None, None, None, None, -1

    # stop if lower bin would contain less background than the higher bin
    if bkg_weighted_left < bkg_weighted_right:
        return None, None, None, None, -1
    
    return probe_edges, sigs, bkg_weighted_right, best_edge, best_sig


if __name__ == '__main__':
    
    from rootpy.extern.argparse import ArgumentParser

    parser = ArgumentParser()
    parser.add_argument('--year', type=int, choices=[2011, 2012], default=2012)
    parser.add_argument('--min-bkg', type=int, default=10)
    parser.add_argument('--steps', type=int, default=100)
    parser.add_argument('--systematics', action='store_true', default=False)
    parser.add_argument('--mass', type=int, default=125, choices=range(100, 155, 5))
    parser.add_argument('--procs', type=int, default=-1)
    args = parser.parse_args()
    year = args.year
    mass = args.mass

    analysis = Analysis(year,
                        systematics=args.systematics,
                        qcd_workspace_norm=False,
                        ztt_workspace_norm=False,
                        qcd_shape_systematic=False)

    for category in (Category_Boosted, Category_VBF):
        analysis.normalize(category)
        clf = analysis.get_clf(category, load=True, mass=mass)
        scores = analysis.get_scores(
            clf, category, TARGET_REGION, mode='workspace',
            masses=[mass], systematics=args.systematics)
        min_score, max_score = scores.min_score, scores.max_score

        # nominal scores for convenience
        b = np.concatenate([scores_dict['NOMINAL'][0] for _, scores_dict in scores.bkg_scores])
        bw = np.concatenate([scores_dict['NOMINAL'][1] for _, scores_dict in scores.bkg_scores])
        s = np.concatenate([scores_dict['NOMINAL'][0] for _, scores_dict in scores.all_sig_scores[mass]])
        sw = np.concatenate([scores_dict['NOMINAL'][1] for _, scores_dict in scores.all_sig_scores[mass]])
        min_score = min(np.min(s), np.min(b)) - 1E-8
        max_score = max(np.max(s), np.max(b)) + 1E-8
        s = (s, sw)
        b = (b, bw)
        
        fig = plt.figure()
        ax1 = fig.add_subplot(111)
        ax1.set_ylabel('Significance')
        ax1.set_xlabel('BDT Score')
        ax2 = ax1.twiny()
        ax2.set_xlabel('Number of Fixed-width Bins')
        ax3 = ax1.twinx()
        ax3.set_ylabel('Events')
        ax3.set_yscale('log')

        # plot the distributions
        b_hist = Hist(20, min_score, max_score, color='blue',
                      linewidth=3, linestyle='dashed')
        s_hist = b_hist.Clone(color='red')
        fill_hist(b_hist, *b)
        fill_hist(s_hist, *s)
        rplt.hist(b_hist, axes=ax3, label='Background')
        rplt.hist(s_hist, axes=ax3, label='Signal')

        # poor man's constant width binning
        nfixed_bins = range(1, 21)
        fixed_sigs = map_pool(
            SigProcess,
            [(scores, np.linspace(min_score, max_score, bins + 1))
                for bins in nfixed_bins], 
            fill_empties=True,
            mass=mass,
            systematics=args.systematics,
            n_jobs=args.procs)
        max_fixed_sig = np.max(fixed_sigs)
        max_fixed_nbins = nfixed_bins[np.argmax(fixed_sigs)]
        
        # show significance vs number of equal width bins
        ax2.plot(nfixed_bins, fixed_sigs, label='Fixed-width Bins', color='green', linestyle='-')

        # demonstrate smart binning
        from itertools import cycle
        lines = ["-", "--", "-.", ":"]
        linecycler = cycle(lines)

        # show significance vs middle bin edge location
        binning = [min_score, max_score]
        prev_best_sig = 0
        nbkg = 0
        while True:
            # linear scan
            edges, sigs, nbkg, best_edge, best_sig = get_best_edge(
                scores, binning,
                steps=args.steps,
                min_bkg_unweighted=args.min_bkg,
                # use nbkg from previous bin so background increases to left
                min_bkg_weighted=nbkg,
                mass=mass,
                systematics=args.systematics,
                n_jobs=args.procs)
            if best_sig <= 0:
                # no valid edge found
                break
            if (best_sig - prev_best_sig) / best_sig < 0.005:
                # improvement is not worth the new bin
                break
            ax1.plot(edges, sigs, color='black', linestyle=next(linecycler))
            ax1.plot((best_edge, best_edge), (0, abs(best_sig)),
                     color='black', linestyle='-', linewidth=2)
            binning.insert(1, best_edge)
            prev_best_sig = best_sig

        # Try to insert an edge in the highest bin.
        # Sometimes there is a local maximum above the first inserted bin edge
        # from above. Adding a bin edge here will usually improve the significance.
        edges, sigs, nbkg, best_edge, best_sig = get_best_edge(
            scores, binning,
            pos=-2,
            steps=args.steps,
            min_bkg_unweighted=args.min_bkg,
            min_bkg_weighted=0,
            mass=mass,
            systematics=args.systematics,
            n_jobs=args.procs)
        if best_sig > prev_best_sig:
            ax1.plot(edges, sigs, color='red', linestyle=next(linecycler))
            ax1.plot((best_edge, best_edge), (0, abs(best_sig)),
                     color='red', linestyle='-', linewidth=2)
            binning.insert(-1, best_edge)
            prev_best_sig = best_sig

        # try inserting N bins into the first bin
        #max_sig = prev_best_sig
        #best_binning = binning
        #best_const_edges = None
        #for n in xrange(2, 10):
        #    const_edges = np.linspace(binning[0], binning[1], n, endpoint=False)[1:]
        #    new_binning = binning[:]
        #    new_binning[1:1] = const_edges
        #    sig = get_sig(scores, new_binning, edge=None, pos=1,
        #                  mass=mass,
        #                  systematics=args.systematics)
        #    if sig > max_sig:
        #        max_sig = sig
        #        best_binning = new_binning
        #        best_const_edges = const_edges
        #for edge in best_const_edges:
        #    ax1.plot((edge, edge), (0, abs(max_sig)),
        #             color='red', linestyle='-', linewidth=2)

        #handles1, labels1 = ax1.get_legend_handles_labels()
        #handles2, labels2 = ax2.get_legend_handles_labels()
        #handles3, labels3 = ax3.get_legend_handles_labels()
        #ax2.legend(handles1+handles2+handles3, labels1+labels2+labels3)
        plt.tight_layout()
        fig.savefig('plots/binning/binning_{0}_{1}_{2}.png'.format(
                    category.name, mass, year % 1000))
        
        # avoid overflow
        binning[0] -= 1E5
        binning[-1] += 1E5

        # save the binning
        with open(os.path.join(CACHE_DIR, 'binning/binning_{0}_{1}_{2}.pickle'.format(
                               category.name, mass, year % 1000)), 'w') as f:
            pickle.dump(binning, f)
