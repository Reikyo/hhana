#!/usr/bin/env python
import os
from math import log
import pickle

from rootpy.plotting import Hist, Canvas
from rootpy.plotting import root2matplotlib as rplt
from rootpy.stats.histfactory import (
    Data, Sample, Channel, make_measurement, make_workspace)

from root_numpy import fill_hist
import numpy as np
import matplotlib.pyplot as plt

from mva import CONST_PARAMS, CACHE_DIR
from mva.categories import Category_VBF, Category_Boosted
from mva.analysis import Analysis
from mva.defaults import TARGET_REGION

from statstools.fixups import fix_measurement
from statstools import get_significance_workspace

#from joblib import Parallel, delayed
import multiprocessing
from multiprocessing import Process
import time


class SigProcess(Process):
    def __init__(self, *args, **kwargs):
        super(SigProcess, self).__init__()
        self.args = args
        self.kwargs = kwargs
        self.result = multiprocessing.Queue()
    
    @property
    def output(self):
        return self.result.get()

    def run(self):
        self.result.put(get_sig(*self.args, **self.kwargs))


def run_pool(workers, n_jobs=12):
    # defensive copy
    workers = workers[:]
    if n_jobs < 1:
        n_jobs = multiprocessing.cpu_count()
    processes = []
    p = None
    try:
        while True:
            active = multiprocessing.active_children()
            while len(active) < n_jobs and len(workers) > 0:
                p = workers.pop(0)
                p.start()
                processes.append(p)
                active = multiprocessing.active_children()
            if len(workers) == 0 and len(active) == 0:
                break
            time.sleep(0.1)
    except KeyboardExit, SystemExit:
        if p is not None:
            p.terminate()
        for p in processes:
            p.terminate()
        raise


def map_pool(process, args, n_jobs=-1, **kwargs):
    procs = [process(*arg, **kwargs) for arg in args]
    run_pool(procs, n_jobs=n_jobs)
    return [p.output for p in procs]


def get_best_edge(scores, edges, pos=0,
                  steps=100, min_bkg=10,
                  mass=125,
                  systematics=False):
    left, right = edges[pos:pos+2]
    # Only consider edges that give non-negative yields for all backgrounds
    # (negative weights on the background events can be present)
    # and edges that will hold at least unweighted min_bkg events in the
    # new bin.
    bkg_weighted = []
    bkg_unweighted = []
    sums_weighted = []
    for bkg, scores_dict in scores.bkg_scores:
        weighted = Hist(steps, left, right)
        unweighted = weighted.Clone()
        s, w = scores_dict['NOMINAL']
        fill_hist(weighted, s, w)
        fill_hist(unweighted, s)
        bkg_weighted.append(weighted)
        bkg_unweighted.append(unweighted)
        # reverse cumsum
        sums_weighted.append(
            np.cumsum(np.array(list(weighted.y()))[::-1])[::-1])
    tot_unweighted = sum(bkg_unweighted)
    sum_unweighted = np.cumsum(np.array(list(tot_unweighted.y()))[::-1])[::-1]
    all_positive = np.logical_and.reduce([b >= 0. for b in sums_weighted])
    if not np.any(all_positive):
        return None, None, None, None, -1
    sum_unweighted_valid = sum_unweighted >= min_bkg
    if not np.any(sum_unweighted_valid):
        return None, None, None, None, -1
    last_bin_min_bkg = np.where(sum_unweighted_valid)[-1][-1]
    # bump last bin down until each background is positive
    last_bin_min_bkg -= all_positive[:last_bin_min_bkg + 1][::-1].argmax()
    last_bin_edge = bkg_weighted[0].xedges(int(last_bin_min_bkg))
    probe_edges = np.linspace(left, last_bin_edge, steps)[1:]
    sigs = map_pool(SigProcess,
                    [(scores, edges, x, pos + 1) for x in probe_edges],
                    mass=mass,
                    systematics=systematics)
    best_sig = np.max(sigs[10:])
    best_edge_idx = np.argmax(sigs[10:]) + 10
    best_edge = probe_edges[best_edge_idx]
    # get unweighted number of background events in this new bin
    bkg_unweighted = []
    for bkg, scores_dict in scores.bkg_scores:
        unweighted = Hist([left, best_edge, right])
        s, w = scores_dict['NOMINAL']
        fill_hist(unweighted, s)
        bkg_unweighted.append(unweighted)
    tot_unweighted = sum(bkg_unweighted)
    # stop if next bin would be too small
    if tot_unweighted[1].value < min_bkg:
        return None, None, None, None, -1
    nbkg = tot_unweighted[2].value
    assert(nbkg >= min_bkg)
    return probe_edges, sigs, nbkg, best_edge, best_sig


def get_workspace(scores, binning,
                  mass=125,
                  fill_empties=False,
                  systematics=False):
    hist_template = Hist(binning)
    background = []
    for sample, scores_dict in scores.bkg_scores:
        background.append(sample.get_histfactory_sample(
            hist_template, None, category, TARGET_REGION, scores=scores_dict,
            systematics=systematics))
    signal = []
    for sample, scores_dict in scores.all_sig_scores[mass]:
        signal.append(sample.get_histfactory_sample(
            hist_template, None, category, TARGET_REGION, scores=scores_dict,
            systematics=systematics))
    data_hist = sum([b.hist for b in background])
    data_hist.name = 'Data'
    data = Data('Data', data_hist)
    channel = Channel(category.name, signal + background, data)
    measurement = make_measurement('MVA', channel,
        POI='SigXsecOverSM',
        const_params=CONST_PARAMS)
    fix_measurement(measurement, fill_empties=fill_empties)
    return make_workspace(measurement, silence=False)


def get_sig(scores, binning, edge=None, pos=1,
            mass=125,
            fill_empties=False,
            systematics=False):
    if edge is not None:
        binning = binning[:]
        binning.insert(pos, edge)
    ws = get_workspace(scores, binning,
                       mass=mass,
                       fill_empties=fill_empties,
                       systematics=systematics)
    hist = get_significance_workspace(ws)
    sig = hist[2].value
    # handle nan
    return 0 if sig != sig else sig


if __name__ == '__main__':
    
    from rootpy.extern.argparse import ArgumentParser

    parser = ArgumentParser()
    parser.add_argument('--year', type=int, choices=[2011, 2012], default=2012)
    parser.add_argument('--min-bkg', type=int, default=10)
    parser.add_argument('--steps', type=int, default=100)
    parser.add_argument('--growth', type=float, default=0.1)
    parser.add_argument('--systematics', action='store_true', default=False)
    parser.add_argument('--mass', type=int, default=125, choices=range(100, 155, 5))
    args = parser.parse_args()
    year = args.year
    mass = args.mass

    analysis = Analysis(year,
                        systematics=args.systematics,
                        qcd_workspace_norm=False,
                        ztt_workspace_norm=False)

    for category in (Category_Boosted, Category_VBF):
        analysis.normalize(category)
        clf = analysis.get_clf(category, load=True, mass=mass)
        scores = analysis.get_scores(
            clf, category, TARGET_REGION, mode='workspace',
            masses=[mass], systematics=args.systematics)
        min_score, max_score = scores.min_score, scores.max_score

        # nominal scores for convenience
        b = np.concatenate([scores_dict['NOMINAL'][0] for _, scores_dict in scores.bkg_scores])
        bw = np.concatenate([scores_dict['NOMINAL'][1] for _, scores_dict in scores.bkg_scores])
        s = np.concatenate([scores_dict['NOMINAL'][0] for _, scores_dict in scores.all_sig_scores[mass]])
        sw = np.concatenate([scores_dict['NOMINAL'][1] for _, scores_dict in scores.all_sig_scores[mass]])
        min_score = min(np.min(s), np.min(b)) - 1E-8
        max_score = max(np.max(s), np.max(b)) + 1E-8
        s = (s, sw)
        b = (b, bw)
        
        fig = plt.figure()
        ax1 = fig.add_subplot(111)
        ax1.set_ylabel('Significance')
        ax1.set_xlabel('BDT Score')
        ax2 = ax1.twiny()
        ax2.set_xlabel('Number of Fixed-width Bins')
        ax3 = ax1.twinx()
        ax3.set_ylabel('Events')
        ax3.set_yscale('log')

        # plot the distributions
        b_hist = Hist(20, min_score, max_score, color='blue',
                    linewidth=3, linestyle='dashed')
        s_hist = b_hist.Clone(color='red')
        fill_hist(b_hist, *b)
        fill_hist(s_hist, *s)
        rplt.hist(b_hist, axes=ax3, label='Background')
        rplt.hist(s_hist, axes=ax3, label='Signal')

        # poor man's constant width binning
        nfixed_bins = range(1, 21)
        fixed_sigs = map_pool(
            SigProcess,
            [(scores, np.linspace(min_score, max_score, bins + 1))
                for bins in nfixed_bins], 
            fill_empties=True,
            mass=mass,
            systematics=args.systematics)
        max_fixed_sig = np.max(fixed_sigs)
        max_fixed_nbins = nfixed_bins[np.argmax(fixed_sigs)]
        
        # show significance vs number of equal width bins
        ax2.plot(nfixed_bins, fixed_sigs, label='Fixed-width Bins', color='green', linestyle='-')

        # demonstrate smart binning
        from itertools import cycle
        lines = ["-", "--", "-.", ":"]
        linecycler = cycle(lines)

        # show significance vs middle bin edge location
        binning = [min_score, max_score]
        prev_best_sig = 0
        min_bkg = args.min_bkg
        while True:
            # linear scan
            edges, sigs, nbkg, best_edge, best_sig = get_best_edge(
                scores, binning,
                steps=args.steps,
                min_bkg=min_bkg,
                systematics=args.systematics)
            if best_sig <= 0:
                # no valid edge found
                break
            if (best_sig - prev_best_sig) / best_sig < 0.005:
                # improvement is not worth the new bin
                break
            ax1.plot(edges, sigs, color='black', linestyle=next(linecycler))
            binning.insert(1, best_edge)
            ax1.plot((best_edge, best_edge), (0, abs(best_sig)),
                     color='black', linestyle='-', linewidth=2)
            prev_best_sig = best_sig
            min_bkg = int(max(args.min_bkg, nbkg * args.growth))

        #handles1, labels1 = ax1.get_legend_handles_labels()
        #handles2, labels2 = ax2.get_legend_handles_labels()
        #handles3, labels3 = ax3.get_legend_handles_labels()
        #ax2.legend(handles1+handles2+handles3, labels1+labels2+labels3)
        plt.tight_layout()
        fig.savefig('binning_{0}_{1}_{2}.png'.format(
                    category.name, mass, year % 1000))
        
        # avoid overflow
        binning[0] -= 1E5
        binning[-1] += 1E5

        # save the binning
        with open(os.path.join(CACHE_DIR, 'binning_{0}_{1}_{2}.pickle'.format(
                               category.name, mass, year % 1000)), 'w') as f:
            pickle.dump(binning, f)
