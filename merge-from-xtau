#!/usr/bin/env python

import os
from hhdb.datasets import Database

import re
import os
from glob import glob
from rootpy.io import root_open as ropen
from rootpy.tree import Tree, TreeChain
import ROOT
import logging
from rootpy import asrootpy

log = logging.getLogger(os.path.basename(__file__))


TREENAME = 'NOMINAL'
CUTFLOW_NAME = 'cutflow_HSM_lephad_NOMINAL'
DAOD_CUTFLOW = 'h_mc_derivation'

def update_merged_file(
    dataset, outfilename, 
    treename='NOMINAL', 
    cutflow_name='cutflow_HSM_lephad_NOMINAL',
    daod_cutflow_name='h_mc_derivation'):
    log.info('{0} ...'.format(dataset.name))
    # intree = TreeChain(name=treename, files=dataset.files)
    intree = ROOT.TChain(treename)
    for f in dataset.files:
        intree.Add(f)
    log.info(intree)
    # instantiate the cutflow histo
    # outcutflow = ropen(dataset.files[0])[cutflow_name].Clone()
    # outcutflow.Reset()
    # outcutflow.name = dataset.name + '_cutflow'
    if (intree.GetEntries() == 0):
      return None  
    outdaod = ropen(dataset.files[0])[daod_cutflow_name].Clone()
    outdaod.Reset()
    outdaod.name = dataset.name + '_daod'

    log.info('merging in %s' % outname)
    with ropen(outname, 'UPDATE') as outfile:
        outfile.cd()
        print outfile, type(outfile)
        outtree = intree.CloneTree(-1, "fast SortBasketsByEntry")
        outtree.OptimizeBaskets()
        outtree.SetName(dataset.name.replace('-', '_'))
        outtree.Write(outtree.GetName(), ROOT.TObject.kOverwrite)
        for f in dataset.files:
            with ropen(f, 'READ') as infile:
                # outcutflow += infile[cutflow_name]
                outdaod += infile[daod_cutflow_name]

        outfile.cd()
        # outcutflow.Write(outcutflow.GetName(), ROOT.TObject.kOverwrite)
        outdaod.Write(outdaod.GetName(), ROOT.TObject.kOverwrite)




if __name__ == '__main__':

    from argparse import ArgumentParser
    parser = ArgumentParser()
    parser.add_argument('-o', '--output', default='output.root')
    parser.add_argument('--db', default='datasets_lh')
    parser.add_argument('--reset', action='store_true', default=False)
    parser.add_argument('--cutflow', default='cutflow_HTauTauLepHad_NOMINAL')
    args = parser.parse_args()

    DB = Database(args.db)
    if args.output is not None:
        outname = args.output
    else:
        # this has to be updated
        outname = '/afs/cern.ch/work/m/mayoub/prod_v2/2015-09-22/lhskim.root'

    if args.reset:
        if os.path.exists(outname):
            log.warning('Deleting %s' % outname)
            os.remove(outname)

    for d in DB.keys():
        dataset = DB[d]
        if len(dataset.files) == 0:
            log.warning(dataset)
            continue
        log.info(dataset)
        # when dealing with systematics 
        # we will loop over different values of 
        # treename and cutflow_name
        update_merged_file(
            dataset, outname, 
            treename='NOMINAL', 
            cutflow_name=args.cutflow)

