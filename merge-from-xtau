#!/usr/bin/env python

import os
from hhdb.datasets import Database

import re
import os
from glob import glob
from rootpy.io import root_open as ropen
from rootpy.tree import Tree, TreeChain
import ROOT
import logging


log = logging.getLogger(os.path.basename(__file__))


TREENAME = 'NOMINAL'
CUTFLOW_NAME = 'cutflow_HSM_lephad_NOMINAL'
DAOD_CUTFLOW = 'h_mc_derivation'

def update_merged_file(
    dataset, outfilename, 
    treename='NOMINAL', 
    cutflow_name='cutflow_HSM_lephad_NOMINAL',
    daod_cutflow_name='h_mc_derivation'):
    log.info('{0} ...'.format(dataset.name))
    # intree = TreeChain(name=treename, files=dataset.files)
    intree = ROOT.TChain(treename)
    for f in dataset.files:
        intree.Add(f)

    # instantiate the cutflow histo
    outcutflow = ropen(dataset.files[0])[cutflow_name].Clone()
    outcutflow.Reset()
    outcutflow.name = dataset.name + '_cutflow'

    outdaod = ropen(dataset.files[0])[daod_cutflow_name].Clone()
    outdaod.Reset()
    outdaod.name = dataset.name + '_daod'

    log.info('merging in %s' % outname)
    with ropen(outname, 'UPDATE') as outfile:
        outfile.cd()
        outtree = intree.CloneTree(-1, "fast SortBasketsByEntry")
        outtree.OptimizeBaskets()
        outtree.SetName(dataset.name.replace('-', '_'))
        outtree.Write(outtree.GetName(), ROOT.TObject.kOverwrite)
        for f in dataset.files:
            with ropen(f, 'READ') as infile:
                outcutflow += infile[cutflow_name]
                outdaod += infile[daod_cutflow_name]

        outfile.cd()
        outcutflow.Write(outcutflow.GetName(), ROOT.TObject.kOverwrite)
        outdaod.Write(outdaod.GetName(), ROOT.TObject.kOverwrite)




if __name__ == '__main__':

    from argparse import ArgumentParser
    parser = ArgumentParser()
    parser.add_argument('-o', '--output', default='output.root')
    parser.add_argument('--db', default='datasets_lh')
    parser.add_argument('--reset', action='store_true', default=False)
    args = parser.parse_args()

    DB = Database(args.db)
    if args.output is not None:
        outname = args.output
    else:
        # this has to be updated
        outname = '/afs/cern.ch/user/q/qbuat/work/public/xtau_output/lephad/v1_1/merged/lhskim.root'

    if args.reset:
        if os.path.exists(outname):
            log.warning('Deleting %s' % outname)
            os.remove(outname)

    for d in DB.keys():
        dataset = DB[d]
        if len(dataset.files) == 0:
            log.warning(dataset)
            continue

        # when dealing with systematics 
        # we will loop over different values of 
        # treename and cutflow_name
        update_merged_file(
            dataset, outname, 
            treename='NOMINAL', 
            cutflow_name='cutflow_HSM_lephad_NOMINAL')

